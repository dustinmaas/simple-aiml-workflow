{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "330f0ed8",
   "metadata": {},
   "source": [
    "# Network Metrics Analysis and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f78509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and setup\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import datasets\n",
    "import huggingface_hub as hf\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import datetime\n",
    "import json\n",
    "import shutil\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "# Get Hugging Face token from environment variable\n",
    "HF_TOKEN = os.environ.get(\"HF_TOKEN\")\n",
    "if not HF_TOKEN:\n",
    "    print(\"Warning: HF_TOKEN environment variable not found. You may need to set it for Hugging Face operations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc45080c",
   "metadata": {},
   "source": [
    "## 1. Creating and Uploading Datasets to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81859515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original dataset\n",
    "df = pd.read_csv('/app/data/data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bff025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the pandas DataFrame to a Hugging Face Dataset\n",
    "dataset = datasets.Dataset.from_pandas(df)\n",
    "\n",
    "# Display basic information about the dataset\n",
    "print(f\"Dataset has {len(dataset)} rows and the following features:\")\n",
    "print(dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3a52b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the repository name for your dataset\n",
    "repo_name = \"cyberpowder/cyberpowder-network-metrics\"\n",
    "\n",
    "# Create the repository if it doesn't exist\n",
    "hf_api = hf.HfApi()\n",
    "try:\n",
    "    hf_api.create_repo(repo_name, private=True, token=HF_TOKEN)\n",
    "    print(f\"Created new repository: {repo_name}\")\n",
    "except Exception as e:\n",
    "    print(f\"Repository may already exist or error creating it: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86e3617",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the dataset to the Hugging Face Hub\n",
    "dataset.push_to_hub(\n",
    "    repo_name,\n",
    "    private=True,  # Set to False to make it publicly accessible\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print(f\"Dataset successfully uploaded to https://huggingface.co/datasets/{repo_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b537f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a filtered dataset for UE1\n",
    "df = dataset.to_pandas()\n",
    "\n",
    "# Filter for ue_id 1 only\n",
    "print(f\"Original dataset size: {len(df)} rows\")\n",
    "df = df[df['ue_id'] == 1]\n",
    "print(f\"After filtering for ue_id 1: {len(df)} rows\")\n",
    "\n",
    "# Create a new dataset from the processed DataFrame\n",
    "ml_dataset = datasets.Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6bb864",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Push the UE1 dataset with the ML-ready configuration\n",
    "ml_dataset.push_to_hub(\n",
    "    repo_name,\n",
    "    config_name=\"ue1_ml_ready\",  # Creates a new configuration in the same repo\n",
    "    private=True,\n",
    "    token=HF_TOKEN,\n",
    ")\n",
    "\n",
    "print(f\"UE1 ML-ready dataset configuration successfully pushed to {repo_name}\")\n",
    "print(f\"To load this specific configuration: datasets.load_dataset('{repo_name}', 'ue1_ml_ready')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90693031",
   "metadata": {},
   "source": [
    "## 2. Downloading Datasets from Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def0abaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset from Hugging Face\n",
    "dataset_ue1 = datasets.load_dataset(repo_name, 'ue1_ml_ready', token=HF_TOKEN)\n",
    "print(f\"Successfully loaded dataset with {len(dataset_ue1['train'])} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4e9496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas and prepare for analysis\n",
    "df = dataset_ue1['train'].to_pandas()\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "print(\"Dataset statistics:\")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df96f52a",
   "metadata": {},
   "source": [
    "## 3. Visualizing the UE1 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b0d276",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Time series visualization of key metrics\n",
    "fig = go.Figure()\n",
    "\n",
    "# Add each metric as a separate trace\n",
    "fig.add_trace(go.Scatter(x=df['timestamp'], y=df['atten'], mode='lines', name='atten'))\n",
    "fig.add_trace(go.Scatter(x=df['timestamp'], y=df['CQI'], mode='lines', name='CQI'))\n",
    "fig.add_trace(go.Scatter(x=df['timestamp'], y=df['RSRP'], mode='lines', name='RSRP'))\n",
    "fig.add_trace(go.Scatter(x=df['timestamp'], y=df['DRB.UEThpDl'] / 1000.0, mode='lines', name='DRB.UEThpDl (Mbps)'))\n",
    "fig.add_trace(go.Scatter(x=df['timestamp'], y=df['min_prb_ratio'], mode='lines', name='min_prb_ratio'))\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Time Series of Network Metrics',\n",
    "    xaxis_title='Timestamp',\n",
    "    yaxis_title='Value',\n",
    "    legend_title='Metrics',\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "# Add range slider\n",
    "fig.update_layout(\n",
    "    xaxis=dict(\n",
    "        rangeselector=dict(\n",
    "            buttons=list([\n",
    "                dict(count=1, label=\"1h\", step=\"hour\", stepmode=\"backward\"),\n",
    "                dict(count=6, label=\"6h\", step=\"hour\", stepmode=\"backward\"),\n",
    "                dict(count=12, label=\"12h\", step=\"hour\", stepmode=\"backward\"),\n",
    "                dict(count=1, label=\"1d\", step=\"day\", stepmode=\"backward\"),\n",
    "                dict(step=\"all\")\n",
    "            ])\n",
    "        ),\n",
    "        rangeslider=dict(visible=True),\n",
    "        type=\"date\"\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c38bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Throughput vs. CQI by min_prb_ratio\n",
    "def make_scatter_for_prb(df, prb_value):\n",
    "    df_filtered = df[df['min_prb_ratio'] == prb_value]\n",
    "    return go.Scatter(\n",
    "        x=df_filtered['CQI'],\n",
    "        y=df_filtered['DRB.UEThpDl'],\n",
    "        mode='markers',\n",
    "        name=f'min_prb_ratio = {prb_value}',\n",
    "        marker=dict(\n",
    "            size=8,\n",
    "            opacity=0.7,\n",
    "        ),\n",
    "        hovertemplate='CQI: %{x}<br>Throughput: %{y:.2f} Mbps<extra></extra>'\n",
    "    )\n",
    "\n",
    "# Get unique min_prb_ratio values\n",
    "unique_prb_values = sorted(df['min_prb_ratio'].unique())\n",
    "\n",
    "# Create subplot grid with one subplot per min_prb_ratio value\n",
    "fig = make_subplots(\n",
    "    rows=1, \n",
    "    cols=len(unique_prb_values),\n",
    "    subplot_titles=[f'min_prb_ratio = {val}' for val in unique_prb_values],\n",
    "    shared_yaxes=True\n",
    ")\n",
    "\n",
    "# Add a scatter trace for each min_prb_ratio value\n",
    "for i, prb_value in enumerate(unique_prb_values):\n",
    "    fig.add_trace(\n",
    "        make_scatter_for_prb(df, prb_value),\n",
    "        row=1, \n",
    "        col=i+1\n",
    "    )\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Throughput vs. CQI by min_prb_ratio',\n",
    "    height=500,\n",
    "    width=200 * len(unique_prb_values),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Update axes labels\n",
    "for i in range(len(unique_prb_values)):\n",
    "    fig.update_xaxes(title_text=\"CQI\", row=1, col=i+1)\n",
    "    if i == 0:  # Only add y-axis title to the first subplot\n",
    "        fig.update_yaxes(title_text=\"Throughput (Mbps)\", row=1, col=i+1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c7f176",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Prepare dataset for model training\n",
    "device = 'cpu'\n",
    "\n",
    "dataset_ue1.set_format(type='torch', columns=['CQI', 'DRB.UEThpDl', 'min_prb_ratio'], dtype=torch.float32)\n",
    "\n",
    "# Convert features and targets to PyTorch tensors\n",
    "X = torch.stack([dataset_ue1['train']['CQI'], dataset_ue1['train']['DRB.UEThpDl']], dim=1)\n",
    "y = dataset_ue1['train']['min_prb_ratio'].unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4e8c22",
   "metadata": {},
   "source": [
    "## 4. Linear Regression Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9563f713",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Define the linear regression model\n",
    "class LinearRegressionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LinearRegressionModel, self).__init__()\n",
    "        self.linear = torch.nn.Linear(2, 1)  # two input features, one output feature\n",
    "        \n",
    "        # Apply batch normalization to input features\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(2)\n",
    "        \n",
    "        # Register buffers to store the mean and standard deviation of the output features\n",
    "        self.register_buffer('y_mean', torch.zeros(1))\n",
    "        self.register_buffer('y_std', torch.ones(1))\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x_normalized = self.batch_norm(x)\n",
    "        output = self.linear(x_normalized)\n",
    "        \n",
    "        if not self.training:\n",
    "            with torch.no_grad():\n",
    "                output = output * self.y_std + self.y_mean\n",
    "                \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219a0e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the linear model\n",
    "model = LinearRegressionModel()\n",
    "model.y_mean = y.mean(dim=0, keepdim=True)\n",
    "model.y_std = y.std(dim=0, keepdim=True)\n",
    "model.to(device)\n",
    "X.to(device)\n",
    "y.to(device)\n",
    "criterion = torch.nn.MSELoss() # Mean Squared Error\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=.05)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 500\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    # Forward pass\n",
    "    y_predicted = model(X)\n",
    "    loss = criterion(y_predicted, (y - model.y_mean) / model.y_std)\n",
    "    # Backward and optimize\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if (epoch) % 10 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290eeb52",
   "metadata": {},
   "source": [
    "## 5. Visualizing the Linear Regression Hyperplane Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf1806e9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Examine the hyperplane fit\n",
    "model.eval()\n",
    "\n",
    "# Get the learned parameters\n",
    "learned_weights = model.linear.weight.data.cpu().numpy()\n",
    "learned_bias = model.linear.bias.data.cpu().numpy()\n",
    "\n",
    "# Print the learned hyperplane equation (in scaled space)\n",
    "print(\"\\nLearned Hyperplane (in scaled space):\")\n",
    "print(f\"y_scaled = {learned_weights[0][0]:.2f}*x1_scaled + {learned_weights[0][1]:.2f}*x2_scaled + {learned_bias[0]:.2f}\")\n",
    "\n",
    "# Get feature normalization parameters from the batch normalization layer\n",
    "x_mean = model.batch_norm.running_mean.cpu().numpy().reshape(1, -1)\n",
    "x_std = torch.sqrt(model.batch_norm.running_var).cpu().numpy().reshape(1, -1)\n",
    "\n",
    "# Create scaled versions of features and targets using the model's normalization parameters\n",
    "features_scaled = (X.cpu().numpy() - x_mean) / x_std\n",
    "targets_scaled = (y.cpu().numpy() - model.y_mean.cpu().numpy()) / model.y_std.cpu().numpy()\n",
    "\n",
    "# Get original unscaled data\n",
    "features_unscaled = X.cpu().numpy()\n",
    "targets_unscaled = y.cpu().numpy()\n",
    "\n",
    "# Sample every 5th point for clarity in the scatter plot\n",
    "sample_indices = np.arange(0, len(features_unscaled), 5)\n",
    "features_sampled = features_unscaled[sample_indices]\n",
    "targets_sampled = targets_unscaled[sample_indices]\n",
    "\n",
    "# Create a meshgrid for the hyperplane using unscaled feature ranges\n",
    "x1_range = np.linspace(features_unscaled[:,0].min(), features_unscaled[:,0].max(), 20)\n",
    "x2_range = np.linspace(features_unscaled[:,1].min(), features_unscaled[:,1].max(), 20)\n",
    "X1, X2 = np.meshgrid(x1_range, x2_range)\n",
    "\n",
    "# Convert meshgrid to scaled space for prediction using the batch normalization parameters\n",
    "X1_scaled = (X1 - x_mean[0, 0]) / x_std[0, 0]\n",
    "X2_scaled = (X2 - x_mean[0, 1]) / x_std[0, 1]\n",
    "\n",
    "# Calculate predictions in scaled space\n",
    "Y_predicted_scaled = learned_weights[0][0] * X1_scaled + learned_weights[0][1] * X2_scaled + learned_bias[0]\n",
    "\n",
    "# Convert predictions back to unscaled space\n",
    "Y_predicted_unscaled = Y_predicted_scaled * model.y_std.cpu().numpy()[0, 0] + model.y_mean.cpu().numpy()[0, 0]\n",
    "\n",
    "# Create Plotly figure\n",
    "fig = make_subplots(rows=1, cols=1, specs=[[{'type': 'scene'}]])\n",
    "\n",
    "# Add scatter plot for data points\n",
    "scatter = go.Scatter3d(\n",
    "    x=features_sampled[:,0],\n",
    "    y=features_sampled[:,1],\n",
    "    z=targets_sampled.flatten(),\n",
    "    mode='markers',\n",
    "    marker=dict(\n",
    "        size=2,\n",
    "        color='red',\n",
    "        opacity=0.8\n",
    "    ),\n",
    "    name='Data Points',\n",
    "    hovertemplate='CQI: %{x:.2f}<br>Throughput: %{y:.2f} Mbps<br>min_prb_ratio: %{z:.2f}<extra></extra>'\n",
    ")\n",
    "fig.add_trace(scatter)\n",
    "\n",
    "# Add surface plot for the hyperplane\n",
    "surface = go.Surface(\n",
    "    x=X1, \n",
    "    y=X2, \n",
    "    z=Y_predicted_unscaled,\n",
    "    colorscale='Blues',\n",
    "    opacity=0.7,\n",
    "    showscale=False,\n",
    "    name='Predicted Hyperplane',\n",
    "    hovertemplate='CQI: %{x:.2f}<br>Throughput: %{y:.2f} Mbps<br>Predicted min_prb_ratio: %{z:.2f}<extra></extra>'\n",
    ")\n",
    "fig.add_trace(surface)\n",
    "\n",
    "# Update layout with labels and title\n",
    "fig.update_layout(\n",
    "    title='Linear Regression Hyperplane Fit (Unscaled Values)',\n",
    "    scene=dict(\n",
    "        xaxis_title='CQI',\n",
    "        yaxis_title='DRB.UEThpDL (Mbps)',\n",
    "        zaxis_title='min_prb_ratio',\n",
    "        aspectmode='auto'\n",
    "    ),\n",
    "    legend=dict(\n",
    "        y=0.99,\n",
    "        x=0.01,\n",
    "        font=dict(size=12)\n",
    "    ),\n",
    "    margin=dict(l=0, r=0, b=0, t=30),\n",
    "    width=800,\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baad4acd",
   "metadata": {},
   "source": [
    "## 6. Polynomial Regression Model Definition and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d422f8d9",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# provide a pathway; leave for students to fill in\n",
    "# Define the polynomial regression model\n",
    "class PolynomialRegressionModel(torch.nn.Module):\n",
    "    def __init__(self, degree=2):\n",
    "        super(PolynomialRegressionModel, self).__init__()\n",
    "        self.degree = degree\n",
    "        \n",
    "        # Calculate number of polynomial features for 2 input features with degree n\n",
    "        # For 2 features with degree 2: x1, x2, x1^2, x1*x2, x2^2 = 5 features\n",
    "        n_poly_features = int((degree + 1) * (degree + 2) / 2) - 1  # -1 because we start from degree 1, not 0\n",
    "        \n",
    "        # Apply batch normalization to expanded polynomial features\n",
    "        self.batch_norm = torch.nn.BatchNorm1d(n_poly_features)\n",
    "        \n",
    "        # Linear layer now accepts polynomial features as input\n",
    "        self.linear = torch.nn.Linear(n_poly_features, 1)\n",
    "        \n",
    "        # Register buffers to store the mean and standard deviation of the output features\n",
    "        self.register_buffer('y_mean', torch.zeros(1))\n",
    "        self.register_buffer('y_std', torch.ones(1))\n",
    "\n",
    "    def _polynomial_features(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate polynomial features up to the specified degree.\n",
    "        For input [x1, x2], with degree=2, this generates [x1, x2, x1^2, x1*x2, x2^2]\n",
    "        \"\"\"\n",
    "        batch_size = x.shape[0]\n",
    "        x1 = x[:, 0].view(-1, 1)\n",
    "        x2 = x[:, 1].view(-1, 1)\n",
    "        \n",
    "        # Start with degree 1 terms (original features)\n",
    "        poly_features = [x1, x2]\n",
    "        \n",
    "        # Add higher degree terms\n",
    "        for d in range(2, self.degree + 1):\n",
    "            for i in range(d + 1):\n",
    "                # Add term x1^(d-i) * x2^i\n",
    "                term = torch.pow(x1, d-i) * torch.pow(x2, i)\n",
    "                poly_features.append(term)\n",
    "        \n",
    "        # Concatenate all features\n",
    "        return torch.cat(poly_features, dim=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # First transform input to polynomial features\n",
    "        x_poly = self._polynomial_features(x)\n",
    "        \n",
    "        # Then apply batch normalization to polynomial features\n",
    "        x_poly_normalized = self.batch_norm(x_poly)\n",
    "        \n",
    "        # Apply linear transformation to normalized polynomial features\n",
    "        output = self.linear(x_poly_normalized)\n",
    "        \n",
    "        # Denormalize output during inference\n",
    "        if not self.training:\n",
    "            with torch.no_grad():\n",
    "                output = output * self.y_std + self.y_mean\n",
    "                \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6867c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train the polynomial regression model\n",
    "poly_model = PolynomialRegressionModel(degree=2)  # Using degree 2 polynomial\n",
    "poly_model.y_mean = y.mean(dim=0, keepdim=True)\n",
    "poly_model.y_std = y.std(dim=0, keepdim=True)\n",
    "poly_model.to(device)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(poly_model.parameters(), lr=0.01)\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 10000\n",
    "for epoch in range(num_epochs):\n",
    "    poly_model.train()\n",
    "    # Forward pass\n",
    "    y_predicted = poly_model(X)\n",
    "    loss = criterion(y_predicted, (y - poly_model.y_mean) / poly_model.y_std)\n",
    "    \n",
    "    # Backward and optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch) % 20 == 0:\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c90708f",
   "metadata": {},
   "source": [
    "## 7. Comparing Linear and Polynomial Regression Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc1383c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set both models to evaluation mode for fair comparison\n",
    "model.eval()\n",
    "poly_model.eval()\n",
    "\n",
    "# Calculate comprehensive performance metrics\n",
    "with torch.no_grad():\n",
    "    # Get raw predictions from both models\n",
    "    linear_preds = model(X)\n",
    "    poly_preds = poly_model(X)\n",
    "    \n",
    "    # Calculate MSE directly against raw targets\n",
    "    linear_mse = torch.nn.functional.mse_loss(linear_preds, y).item()\n",
    "    poly_mse = torch.nn.functional.mse_loss(poly_preds, y).item()\n",
    "    \n",
    "    # Calculate R² score (coefficient of determination)\n",
    "    y_mean = torch.mean(y)\n",
    "    total_variance = torch.sum((y - y_mean)**2)\n",
    "    linear_residual_variance = torch.sum((y - linear_preds)**2)\n",
    "    poly_residual_variance = torch.sum((y - poly_preds)**2)\n",
    "    \n",
    "    linear_r2 = (1 - linear_residual_variance / total_variance).item()\n",
    "    poly_r2 = (1 - poly_residual_variance / total_variance).item()\n",
    "    \n",
    "    # Calculate mean absolute error\n",
    "    linear_mae = torch.mean(torch.abs(linear_preds - y)).item()\n",
    "    poly_mae = torch.mean(torch.abs(poly_preds - y)).item()\n",
    "\n",
    "# Print comparison results\n",
    "print(f\"Performance Metrics Comparison:\")\n",
    "print(f\"{'Metric':<20} {'Linear':<15} {'Polynomial':<15} {'Improvement':<15}\")\n",
    "print(f\"{'-'*60}\")\n",
    "print(f\"{'MSE':<20} {linear_mse:<15.6f} {poly_mse:<15.6f} {(1 - poly_mse/linear_mse)*100:<15.2f}%\")\n",
    "print(f\"{'MAE':<20} {linear_mae:<15.6f} {poly_mae:<15.6f} {(1 - poly_mae/linear_mae)*100:<15.2f}%\")\n",
    "print(f\"{'R² Score':<20} {linear_r2:<15.6f} {poly_r2:<15.6f} {(poly_r2 - linear_r2)*100:<15.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecbba87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual comparison of both models\n",
    "model.eval()\n",
    "poly_model.eval()\n",
    "\n",
    "# Create a grid of points for visualization\n",
    "grid_size = 30\n",
    "x1_range = np.linspace(features_unscaled[:,0].min(), features_unscaled[:,0].max(), grid_size)\n",
    "x2_range = np.linspace(features_unscaled[:,1].min(), features_unscaled[:,1].max(), grid_size)\n",
    "X1, X2 = np.meshgrid(x1_range, x2_range)\n",
    "\n",
    "# Flatten the grid points for prediction\n",
    "grid_points = np.column_stack([X1.flatten(), X2.flatten()])\n",
    "grid_tensor = torch.tensor(grid_points, dtype=torch.float32)\n",
    "\n",
    "# Get predictions from both models\n",
    "with torch.no_grad():\n",
    "    poly_predictions = poly_model(grid_tensor).cpu().numpy().reshape(grid_size, grid_size)\n",
    "    linear_predictions = model(grid_tensor).cpu().numpy().reshape(grid_size, grid_size)\n",
    "\n",
    "# Create 3D visualization comparing both models\n",
    "fig = make_subplots(\n",
    "    rows=1, \n",
    "    cols=2,\n",
    "    specs=[[{'type': 'scene'}, {'type': 'scene'}]],\n",
    "    subplot_titles=[\"Linear Regression Surface\", \"Polynomial Regression Surface\"]\n",
    ")\n",
    "\n",
    "# Sample a subset of data points for visualization\n",
    "sample_indices = np.random.choice(len(features_unscaled), size=min(300, len(features_unscaled)), replace=False)\n",
    "sample_features = features_unscaled[sample_indices]\n",
    "sample_targets = targets_unscaled[sample_indices]\n",
    "\n",
    "# Add data points to both subplots\n",
    "for i in range(1, 3):\n",
    "    fig.add_trace(\n",
    "        go.Scatter3d(\n",
    "            x=sample_features[:,0],\n",
    "            y=sample_features[:,1],\n",
    "            z=sample_targets.flatten(),\n",
    "            mode='markers',\n",
    "            marker=dict(\n",
    "                size=3,\n",
    "                color='red',\n",
    "                opacity=0.5\n",
    "            ),\n",
    "            name='Data Points',\n",
    "            showlegend=False,\n",
    "            hovertemplate='CQI: %{x:.2f}<br>Throughput: %{y:.2f} Mbps<br>min_prb_ratio: %{z:.2f}<extra></extra>'\n",
    "        ),\n",
    "        row=1, col=i\n",
    "    )\n",
    "\n",
    "# Add linear regression surface\n",
    "fig.add_trace(\n",
    "    go.Surface(\n",
    "        x=X1, \n",
    "        y=X2, \n",
    "        z=linear_predictions,\n",
    "        colorscale='Blues',\n",
    "        opacity=0.7,\n",
    "        showscale=False,\n",
    "        name='Linear Regression'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Add polynomial regression surface\n",
    "fig.add_trace(\n",
    "    go.Surface(\n",
    "        x=X1, \n",
    "        y=X2, \n",
    "        z=poly_predictions,\n",
    "        colorscale='Greens',\n",
    "        opacity=0.7,\n",
    "        showscale=False,\n",
    "        name='Polynomial Regression'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='Comparison of Linear vs. Polynomial Regression Models',\n",
    "    height=600,\n",
    "    width=1200,\n",
    ")\n",
    "\n",
    "# Update scene settings for both subplots\n",
    "for i in range(1, 3):\n",
    "    fig.update_scenes(\n",
    "        xaxis_title='CQI',\n",
    "        yaxis_title='Throughput (Mbps)',\n",
    "        zaxis_title='min_prb_ratio',\n",
    "        aspectmode='auto',\n",
    "        row=1, col=i\n",
    "    )\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3759066",
   "metadata": {},
   "source": [
    "## 8. Uploading the Polynomial Regression Model to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39ae007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the polynomial regression model to ONNX and upload to Hugging Face\n",
    "poly_model_name = \"polynomial_regression_model\"\n",
    "poly_model_version = \"1.0.0\"  # First version\n",
    "\n",
    "# Calculate comprehensive metrics for metadata\n",
    "with torch.no_grad():\n",
    "    poly_preds = poly_model(X)\n",
    "    poly_mse = torch.nn.functional.mse_loss(poly_preds, y).item()\n",
    "    poly_mae = torch.mean(torch.abs(poly_preds - y)).item()\n",
    "    \n",
    "    # Calculate R² score (coefficient of determination)\n",
    "    y_mean = torch.mean(y)\n",
    "    total_variance = torch.sum((y - y_mean)**2)\n",
    "    poly_residual_variance = torch.sum((y - poly_preds)**2)\n",
    "    poly_r2 = (1 - poly_residual_variance / total_variance).item()\n",
    "\n",
    "# Metadata with polynomial degree information and comprehensive metrics\n",
    "poly_metadata_props = {\n",
    "    \"version\": poly_model_version,\n",
    "    \"training_date\": datetime.datetime.now().isoformat(),\n",
    "    \"framework\": f\"PyTorch {torch.__version__}\",\n",
    "    \"dataset\": \"network_metrics_exp_1741030459\",\n",
    "    \"metrics\": json.dumps({\n",
    "        \"mse\": poly_mse,\n",
    "        \"mae\": poly_mae,\n",
    "        \"r2\": poly_r2\n",
    "    }),\n",
    "    \"description\": f\"Polynomial regression model (degree {poly_model.degree}) for PRB prediction based on CQI and throughput\",\n",
    "    \"input_features\": json.dumps([\"CQI\", \"DRB.UEThpDl\"]),\n",
    "    \"output_features\": json.dumps([\"min_prb_ratio\"]),\n",
    "    \"polynomial_degree\": poly_model.degree,\n",
    "    \"model_type\": \"polynomial_regression\"\n",
    "}\n",
    "\n",
    "# Create temp directory\n",
    "poly_temp_dir = tempfile.mkdtemp()\n",
    "poly_model_path = os.path.join(poly_temp_dir, f\"{poly_model_name}_v{poly_model_version}.onnx\")\n",
    "\n",
    "# Export the model to ONNX\n",
    "dummy_input = torch.randn(1, 2)  # Example input\n",
    "torch.onnx.export(\n",
    "    poly_model, \n",
    "    dummy_input, \n",
    "    poly_model_path, \n",
    "    verbose=True, \n",
    "    input_names=[\"input\"], \n",
    "    output_names=[\"output\"], \n",
    "    dynamic_axes={\"input\": {0: \"batch_size\"}, \"output\": {0: \"batch_size\"}}\n",
    ")\n",
    "\n",
    "# Create repository for polynomial model\n",
    "poly_model_repo = f\"cyberpowder/{poly_model_name}_v{poly_model_version}\"\n",
    "\n",
    "# Create metadata JSON file\n",
    "poly_metadata_path = os.path.join(poly_temp_dir, f\"{poly_model_name}_v{poly_model_version}_metadata.json\")\n",
    "with open(poly_metadata_path, 'w') as f:\n",
    "    json.dump(poly_metadata_props, f, indent=2)\n",
    "\n",
    "# Create or ensure repository exists\n",
    "try:\n",
    "    hf_api.create_repo(poly_model_repo, private=True, token=HF_TOKEN)\n",
    "    print(f\"Created new repository: {poly_model_repo}\")\n",
    "except Exception as e:\n",
    "    print(f\"Repository may already exist or error creating it: {e}\")\n",
    "\n",
    "# Upload ONNX model to Hugging Face\n",
    "print(f\"Uploading polynomial ONNX model to Hugging Face: {poly_model_repo}\")\n",
    "hf_api.upload_file(\n",
    "    path_or_fileobj=poly_model_path,\n",
    "    repo_id=poly_model_repo,\n",
    "    path_in_repo=f\"{poly_model_name}_v{poly_model_version}.onnx\",\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "# Upload metadata to Hugging Face\n",
    "print(f\"Uploading polynomial model metadata to Hugging Face: {poly_model_repo}\")\n",
    "hf_api.upload_file(\n",
    "    path_or_fileobj=poly_metadata_path,\n",
    "    repo_id=poly_model_repo,\n",
    "    path_in_repo=f\"{poly_model_name}_v{poly_model_version}_metadata.json\",\n",
    "    token=HF_TOKEN\n",
    ")\n",
    "\n",
    "print(f\"Polynomial model and metadata successfully uploaded to Hugging Face: {poly_model_repo}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5955f7b7",
   "metadata": {},
   "source": [
    "## Testing the Uploaded Polynomial Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6e2d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO need to fix this to only use possible ranges.\n",
    "# Download and test the uploaded polynomial model\n",
    "poly_model_repo = f\"cyberpowder/{poly_model_name}_v{poly_model_version}\"\n",
    "poly_model_filename = f\"{poly_model_name}_v{poly_model_version}.onnx\"\n",
    "poly_metadata_filename = f\"{poly_model_name}_v{poly_model_version}_metadata.json\"\n",
    "\n",
    "# List files in the repo to confirm upload was successful\n",
    "print(f\"Files in repository {poly_model_repo}:\")\n",
    "model_files = hf_api.list_repo_files(poly_model_repo, token=HF_TOKEN)\n",
    "for file in model_files:\n",
    "    print(f\"  - {file}\")\n",
    "\n",
    "# Create temporary directory for downloaded files\n",
    "poly_download_dir = tempfile.mkdtemp()\n",
    "\n",
    "try:\n",
    "    # Download model from Hugging Face\n",
    "    print(f\"\\nDownloading polynomial model from Hugging Face...\")\n",
    "    poly_model_path = hf_hub_download(\n",
    "        repo_id=poly_model_repo,\n",
    "        filename=poly_model_filename,\n",
    "        token=HF_TOKEN,\n",
    "        local_dir=poly_download_dir\n",
    "    )\n",
    "    \n",
    "    # Download metadata\n",
    "    print(f\"Downloading polynomial model metadata from Hugging Face...\")\n",
    "    poly_metadata_path = hf_hub_download(\n",
    "        repo_id=poly_model_repo,\n",
    "        filename=poly_metadata_filename,\n",
    "        token=HF_TOKEN,\n",
    "        local_dir=poly_download_dir\n",
    "    )\n",
    "    \n",
    "    # Load metadata\n",
    "    with open(poly_metadata_path, 'r') as f:\n",
    "        poly_metadata = json.load(f)\n",
    "    \n",
    "    print(f\"\\nPolynomial model metadata:\")\n",
    "    print(f\"  Version: {poly_metadata.get('version')}\")\n",
    "    print(f\"  Polynomial degree: {poly_metadata.get('polynomial_degree')}\")\n",
    "    print(f\"  Description: {poly_metadata.get('description')}\")\n",
    "    print(f\"  Framework: {poly_metadata.get('framework')}\")\n",
    "    print(f\"  Metrics: {poly_metadata.get('metrics')}\")\n",
    "    \n",
    "    # Load model with ONNX Runtime\n",
    "    print(f\"\\nLoading polynomial model for inference...\")\n",
    "    poly_session = ort.InferenceSession(poly_model_path)\n",
    "    \n",
    "    # Sample data for inference\n",
    "    sample_inputs = [\n",
    "        [5.0, 20.0],   # Low CQI, low throughput\n",
    "        [10.0, 50.0],  # Medium CQI, medium throughput\n",
    "        [15.0, 100.0], # High CQI, high throughput\n",
    "        [8.0, 80.0],   # Medium-low CQI, medium-high throughput\n",
    "        [12.0, 30.0]   # Medium-high CQI, medium-low throughput\n",
    "    ]\n",
    "    \n",
    "    input_tensor = np.array(sample_inputs, dtype=np.float32)\n",
    "    \n",
    "    # Run inference with polynomial model\n",
    "    print(f\"\\nRunning inference with sample data...\")\n",
    "    poly_outputs = poly_session.run(None, {\"input\": input_tensor})\n",
    "    \n",
    "    # Print results as a table\n",
    "    print(\"\\nPrediction Results:\")\n",
    "    print(\"------------------------------------------------------\")\n",
    "    print(\"   CQI   | Throughput (Mbps) | Predicted min_prb_ratio\")\n",
    "    print(\"------------------------------------------------------\")\n",
    "    for i, sample in enumerate(sample_inputs):\n",
    "        print(f\"  {sample[0]:5.1f}  |      {sample[1]:7.1f}     |        {poly_outputs[0][i][0]:7.2f}\")\n",
    "    print(\"------------------------------------------------------\")\n",
    "    \n",
    "    # Create a visualization\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Add points for the samples\n",
    "    fig.add_trace(go.Scatter3d(\n",
    "        x=[sample[0] for sample in sample_inputs],  # CQI\n",
    "        y=[sample[1] for sample in sample_inputs],  # Throughput\n",
    "        z=[pred[0] for pred in poly_outputs[0]],    # Predicted min_prb_ratio\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=8,\n",
    "            color='green',\n",
    "            symbol='diamond'\n",
    "        ),\n",
    "        name='Polynomial Model Predictions',\n",
    "        hovertemplate='CQI: %{x:.1f}<br>Throughput: %{y:.1f} Mbps<br>Predicted PRB: %{z:.1f}%<extra></extra>'\n",
    "    ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f\"Predictions from Polynomial Regression Model (Degree {poly_metadata.get('polynomial_degree')})\",\n",
    "        scene=dict(\n",
    "            xaxis_title='CQI',\n",
    "            yaxis_title='Throughput (Mbps)',\n",
    "            zaxis_title='min_prb_ratio (%)',\n",
    "        ),\n",
    "        width=800,\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error using polynomial model from Hugging Face: {e}\")\n",
    "    \n",
    "finally:\n",
    "    # Clean up\n",
    "    shutil.rmtree(poly_download_dir, ignore_errors=True)\n",
    "    print(\"Test completed and temporary files cleaned up\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519f7a05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
