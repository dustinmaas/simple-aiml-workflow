{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72e02bc3",
   "metadata": {
    "cell_marker": "\"\"\"",
    "lines_to_next_cell": 0
   },
   "source": [
    "# Dataset Analysis and Model Creation/Training\n",
    "\n",
    "\n",
    "1. Brief HuggingFace intro; creating an access token; colab notebook login\n",
    "1. Creating and uploading datasets to HuggingFace\n",
    "1. Downloading datasets from HuggingFace\n",
    "1. Visualizing the dataset\n",
    "1. \"Cleaning\" the dataset\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Join the HuggingFace CyberPowder organization: [link](https://huggingface.co/organizations/cyberpowder/share/VkAxpCJJIebrTqXgdMFxRElyHhnyAJocHQ)\n",
    "- Review Section VI (AI/ML Workflows) of the [NEU ORAN paper](https://utah.instructure.com/courses/1045795/files/170447527?wrap=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78c1350",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# Imports and setup\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import tempfile\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.express as px\n",
    "import datasets\n",
    "import huggingface_hub as hf\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import datetime\n",
    "import json\n",
    "import shutil\n",
    "from huggingface_hub import hf_hub_download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e45622d",
   "metadata": {},
   "source": [
    "## 1. Brief HuggingFace intro; creating an access token; colab notebook login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157e967c",
   "metadata": {},
   "source": [
    "### HuggingFace Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d703887",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Creating an Access Token\n",
    "1. Go to the HuggingFace website: [link](https://huggingface.co/)\n",
    "1. Log in to your account\n",
    "1. Go to your profile settings\n",
    "1. Create a new access token with write access\n",
    "1. Copy the token and save it in a\n",
    "   secure location in case you lose track of it later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fda0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# log in to huggingface\n",
    "# (you'll need to enter the access token you created)\n",
    "hf.login()\n",
    "username = hf.whoami()\n",
    "print(f\"Logged in as {username}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f8f887",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "From here on out, calls to the HuggingFace API should be automatically authenticated with your access token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af24d2e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## 2. Creating and Uploading Datasets to Hugging Face\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea0c8cb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Let's create some random data to use as a dataset\n",
    "# We'll use numpy to generate random features and targets and throw them into a pandas DataFrame\n",
    "random_features = np.random.rand(100, 2)  # 100 samples, 2 features\n",
    "random_targets = np.random.rand(100, 1)  # 100 samples, 1 target\n",
    "df = pd.DataFrame(random_features, columns=[\"feature1\", \"feature2\"])\n",
    "df[\"target\"] = random_targets\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed01c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll use the datasets library to create a dataset from the pandas DataFrame and add some metadata\n",
    "# You could also create a dataset from a csv file, json file, etc.\n",
    "dataset = datasets.Dataset.from_pandas(df)\n",
    "dataset.description = \"A dummy dataset for testing purposes\"\n",
    "dataset.tags = [\"dummy\", \"test\"]\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5938c634",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aca8e28f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "repo_name = f\"{username}/dummy-datasets\"\n",
    "dataset_name = f\"dummy-dataset-{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "# Create a dataset repository\n",
    "import hf.HfApi as hf_api\n",
    "try:\n",
    "    hf_api.create_repo(repo_name)\n",
    "except Exception as e:\n",
    "    print(f\"Error creating repository: {e}\")\n",
    "\n",
    "# Upload the dataset to the repository\n",
    "dataset.push_to_hub(repo_name, config_name=\"full\", private=True)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
