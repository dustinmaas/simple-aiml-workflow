{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e774fd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# + [markdown]\n",
    "\"\"\"\n",
    "# Dataset Analysis and Model Creation/Training\n",
    "\n",
    "\n",
    "1. Brief HuggingFace intro\n",
    "  1. Logging in to HuggingFace\n",
    "  1. Creating and uploading datasets to HuggingFace\n",
    "  1. Downloading datasets from HuggingFace\n",
    "1. Analyzing the O-RAN slicing dataset\n",
    "  1. Loading the dataset\n",
    "  1. Visualizing the dataset\n",
    "  1. Processing the dataset\n",
    "  1. Uploading the processed dataset to HuggingFace\n",
    "\n",
    "### Prerequisites\n",
    "\n",
    "- Read Section VI (AI/ML Workflows) of the [NEU ORAN paper](https://utah.instructure.com/courses/1045795/files/170447527?wrap=1)\n",
    "- Join the [HuggingFace CyberPowder organization](https://huggingface.co/cyberpowder)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b6466d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (various other required packages are already available in the colab environment)\n",
    "!uv -q pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f5f876",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import datetime\n",
    "\n",
    "import huggingface_hub as hf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from plotly.subplots import make_subplots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e931b4e5",
   "metadata": {},
   "source": [
    "## 1. Brief HuggingFace intro\n",
    "### Logging in to HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7b7a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log in to huggingface\n",
    "# (you'll need to enter the access token you created earlier)\n",
    "hf.notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d634fdcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that you're logged in (ignore the warning about adding the token as a Colab secret)\n",
    "username = hf.whoami()['name']\n",
    "print(f\"Logged in as {username}\")\n",
    "# From here on out, calls to the HuggingFace API should be automatically authenticated with your access token."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ceb85fa",
   "metadata": {},
   "source": [
    "## 2. Creating and Uploading Datasets to Hugging Face\n",
    "### Creating a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5a2777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create some random data to use as a dataset\n",
    "# We'll use numpy to generate random features and targets and throw them into a pandas DataFrame\n",
    "random_features = np.random.rand(100, 2)  # 100 samples, 2 features\n",
    "random_targets = np.random.rand(100, 1)  # 100 samples, 1 target\n",
    "df = pd.DataFrame(random_features, columns=[\"feature1\", \"feature2\"])\n",
    "df[\"target\"] = random_targets\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6bda6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we'll use the datasets library to create a dataset from the pandas DataFrame\n",
    "# You could also create a dataset from a csv file, json file, etc.\n",
    "dataset = datasets.Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd1bd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's create a repository on HuggingFace to store the dataset we just created\n",
    "# We'll use the current date and time to make the dataset name unique\n",
    "repo_name = f\"{username}/dummy-datasets\"\n",
    "dataset_name = f\"dummy-dataset-{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "\n",
    "# Create the dataset repository using the HuggingFace API\n",
    "# (this will raise an error if the repository already exists, but we'll ignore that for now)\n",
    "hf_api = hf.HfApi()\n",
    "try:\n",
    "    hf_api.create_repo(repo_name)\n",
    "except Exception as e:\n",
    "    print(f\"Error creating repository: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed607df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we'll push the dataset to the repository we just created.\n",
    "If the dataset already exists, and there are no changes, HuggingFace will not create a new version/commit.\n",
    "We'll call the dataset configuration \"full\" to indicate that it contains all the data.\n",
    "We'll also make the dataset private so that only you can access it.\n",
    "\"\"\"\n",
    "dataset.push_to_hub(repo_name, config_name=\"full\", private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "116f0fe3",
   "metadata": {},
   "source": [
    "## 3. Downloading Datasets from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "837611bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now let's make sure we can download the dataset we just uploaded.\n",
    "We'll first use the HuggingFace API to list the datasets in the repository.\n",
    "\"\"\"\n",
    "my_datasets = hf_api.list_datasets(repo_name)\n",
    "my_datasets\n",
    "\n",
    "\"\"\"\n",
    "You should see the dataset we just uploaded in the list of datasets.\n",
    "It may be the only dataset in the list if you haven't uploaded any others.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df31f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Let's download the dataset we just uploaded using the datasets package.\n",
    "\"\"\"\n",
    "dataset = datasets.load_dataset(repo_name)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512cbda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We can now access the dataset as a dictionary. Since we didn't specify a train/test split when we \n",
    "uploaded the dataset, all of the data is in the \"train\" key in the dataset dictionary.\n",
    "\"\"\"\n",
    "df = dataset['train'].to_pandas()\n",
    "df\n",
    "\n",
    "\"\"\"\n",
    "Now, let's move on to interacting with the O-RAN slicing dataset that we'll be processing today\n",
    "for use in next Friday's session, where you will each create, train, and validate a model using\n",
    "PyTorch.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf847cd1",
   "metadata": {},
   "source": [
    "## 4. Analyzing the O-RAN slicing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b9ec8b",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "outputs": [],
   "source": [
    "# Load the data from the \"default\" O-RAN slicing dataset\n",
    "cp_repo_name = \"cyberpowder/cyberpowder-network-metrics\"\n",
    "oran_dataset = datasets.load_dataset(cp_repo_name, \"default\")\n",
    "oran_dataset\n",
    "\n",
    "%%shell\n",
    "ls"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
